{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "Training set has been buffered.\n",
      "Loaded from checkpoint /models/cifar100_htcnn_1/model.pth\n",
      "Validation Accuracy: 0.010151\n",
      "{'Coarst 1 loss': 0.691909131034256, 'Fine loss': 0.6930971982533974, 'total loss': 1.5938146821678143}\n",
      "Validating...\n",
      "Validation Accuracy: 0.009654\n",
      "{'Level 1 accuracy': 0.06757563694267515, 'Fine accuracy': 0.01064888535031847}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7546b4d85323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    344\u001b[0m                           flush_secs = 5)\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-7546b4d85323>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    297\u001b[0m           \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m           \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m          train_batch=128, val_batch=64, checkpoint=checkpoint_path)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-7546b4d85323>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainset, valset, label_file, output_path, output_fname, start_lr, lr_discount, lr_steps, epoch, train_batch, val_batch, val_at, checkpoint, jud_at, aux_scaler)\u001b[0m\n\u001b[1;32m    282\u001b[0m         writer.add_scalars('Final Accuracy', \n\u001b[1;32m    283\u001b[0m                           \u001b[0mv_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m                           i)\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model has been trained.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_scalars\u001b[0;34m(self, main_tag, tag_scalar_dict, global_step, walltime)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mwalltime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwalltime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mfw_logdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_scalar_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0mfw_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfw_logdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmain_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfw_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_writers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import cv2\n",
    "import models.layers\n",
    "import addons.trees as trees\n",
    "from models.vision import HTCNN, LeNet5\n",
    "\n",
    "def loadData(data_path, data_file):\n",
    "    output = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for ln in f:\n",
    "            fields = ln.rstrip('\\n').split(',')\n",
    "            output.append([os.path.join(data_path,fields[0]), int(fields[1])])\n",
    "    return output\n",
    "            \n",
    "def loadInBatch(ds, r = 0, batchsize = 16, shuffle=False):\n",
    "    output_data = None\n",
    "    aux_labels = []\n",
    "    fine_labels = None\n",
    "    i = 0\n",
    "    ndata = len(ds)\n",
    "    hasDone = False\n",
    "    while i<batchsize:\n",
    "        data_rec = ds[r][0]\n",
    "        img_data = cv2.imread(data_rec)\n",
    "        base_label = ds[r][1]\n",
    "        data_blob = torch.tensor(img_data).float().permute(2,0,1)\n",
    "        if output_data is None:\n",
    "            output_data = torch.zeros(batchsize, img_data.shape[2], img_data.shape[0], img_data.shape[1], device=device)\n",
    "        output_data[i, ...] = data_blob\n",
    "        if aux_labels == []:\n",
    "            j = 0\n",
    "            for lv in lookup_lv_list:\n",
    "                output_label = torch.zeros(batchsize, coarst_dims[j], device=device)\n",
    "                output_label.require_grad = False\n",
    "                aux_labels.append(output_label)\n",
    "                j += 1\n",
    "        if fine_labels is None:\n",
    "            fine_labels = torch.zeros(batchsize, n_fine, device=device)\n",
    "        j = 0\n",
    "        for lv in lookup_lv_list:\n",
    "            up_cls = lookupParent(classTree, base_label, lv)\n",
    "            aux_labels[j].data[i, up_cls] = 1.0\n",
    "            j += 1\n",
    "        fine_labels.data[i, base_label] = 1.0\n",
    "        r += 1\n",
    "        if r >= ndata:\n",
    "            r = 0\n",
    "            hasDone = True\n",
    "            if shuffle:\n",
    "                random.shuffle(ds)\n",
    "        i += 1\n",
    "        \n",
    "    output_data.require_grad = False\n",
    "    fine_labels.require_grad = False\n",
    "    return output_data, aux_labels, fine_labels, r, hasDone\n",
    "        \n",
    "def lookupParent(tree, fine_node, upper_lv=1):\n",
    "    return tree[fine_node][upper_lv-1]\n",
    "\n",
    "def accumulateList(list1, list2):\n",
    "    output = []\n",
    "    for i in range(len(list1)):\n",
    "        output.append((list1[i] + list2[i]) * 0.5)\n",
    "    return output\n",
    "\n",
    "def computeBatchAccuracy(pred, expected):\n",
    "    output = []\n",
    "    n_output = len(pred)\n",
    "    n_batch = pred[0].shape[0]\n",
    "    for i in range(n_output):\n",
    "        local_result = 0.0\n",
    "        for j in range(n_batch):\n",
    "            cls_pred = pred[i][j].argmax()\n",
    "            cls_exp = expected[i][j,...].argmax()\n",
    "            #print((cls_pred, cls_exp))\n",
    "            if cls_pred == cls_exp:\n",
    "                local_result += 1.0\n",
    "        local_result /= n_batch\n",
    "        output.append(local_result)\n",
    "    return output\n",
    "\n",
    "def computeAccuracy(dataset, model, batchsize = 1, withAux = False):\n",
    "    data_count = len(dataset)\n",
    "    ptr = 0\n",
    "    batch_len = int(np.floor(float(data_count)/batchsize))\n",
    "    batch_elen = int(np.ceil(float(data_count)/batchsize))\n",
    "    output = []\n",
    "    aux_output = []\n",
    "    for i in range(batch_len):\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch(dataset, ptr, batchsize)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        if output == []:\n",
    "            output = batch_result\n",
    "        else:\n",
    "            for j in range(len(output)):\n",
    "                output[j] += batch_result[j]\n",
    "        if withAux:\n",
    "            batch_aux_result = computeBatchAccuracy(pred_aux, expected_aux + [expected_fine])\n",
    "            if aux_output == []:\n",
    "                aux_output = batch_aux_result\n",
    "            else:\n",
    "                for j in range(len(aux_output)):\n",
    "                    aux_output[j] += batch_aux_result[j]\n",
    "    if batchsize!=1 and batch_len != batch_elen:\n",
    "        tmp_batchsize = data_count - ptr\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch(dataset, ptr, tmp_batchsize)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        for j in range(len(output)):\n",
    "            output[j] += batch_result[j]\n",
    "            output[j] /= batch_len + 1\n",
    "        if withAux:\n",
    "            batch_aux_result = computeBatchAccuracy(pred_aux, expected_aux + [expected_fine])\n",
    "            for j in range(len(aux_output)):\n",
    "                aux_output[j] /= batch_len + 1\n",
    "    else:\n",
    "        for j in range(len(output)):\n",
    "            output[j] /= data_count\n",
    "        if withAux:\n",
    "            for j in range(len(aux_output)):\n",
    "                aux_output[j] /= data_count\n",
    "        \n",
    "    return output, aux_output\n",
    "\n",
    "def train(trainset, valset, label_file, output_path, output_fname, \n",
    "          start_lr=0.1, lr_discount=0.1, lr_steps=[], epoch=30,\n",
    "          train_batch = 16, val_batch = 16, val_at = 10,\n",
    "          checkpoint = None, jud_at = -1, aux_scaler = 0.3):\n",
    "    \n",
    "    best_v_result = 0.0\n",
    "    model = HTCNN(label_file, with_aux = True, with_fc = True, backbone=backbone,\n",
    "              isCuda=True).cuda()\n",
    "    \n",
    "    \n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name)\n",
    "    \n",
    "    output_filepath = os.path.join(output_path, output_fname)\n",
    "\n",
    "    if checkpoint is not None and os.path.isfile(checkpoint):\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "        print('Loaded from checkpoint %s'%checkpoint)\n",
    "    \n",
    "    #sample, _, _, _, _ = loadInBatch(trainset, batchsize = 1)\n",
    "    #writer.add_graph(model, sample)\n",
    "    #writer.close()\n",
    "    \n",
    "    v_result = 0\n",
    "    \n",
    "    backbone.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_result, aux_val_result = computeAccuracy(valset, model, val_batch, withAux=True)\n",
    "        v_result = val_result[0]\n",
    "        print('Validation Accuracy: %f'%v_result)\n",
    "        #print(aux_val_result)\n",
    "        best_v_result = v_result\n",
    "    \n",
    "    lr = start_lr\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    \n",
    "    # create losses\n",
    "    losses = []\n",
    "    aux_loss_names = []\n",
    "    aux_val_names = []\n",
    "    final_loss = nn.SoftMarginLoss()\n",
    "    for lv in lookup_lv_list:\n",
    "        losses.append(nn.SoftMarginLoss())\n",
    "        aux_loss_names.append('Coarst %d loss'%lv)\n",
    "        aux_val_names.append('Level %d accuracy'%lv)\n",
    "    losses.append(nn.SoftMarginLoss())\n",
    "    aux_loss_names.append('Fine loss')\n",
    "    aux_val_names.append('Fine accuracy')\n",
    "    n_aux = len(losses) - 1\n",
    "    aux_accuracy = {}\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        # training phase\n",
    "        backbone.train()\n",
    "        model.train()\n",
    "        ptr = 0\n",
    "        hasFinishEpoch = False\n",
    "        epoch_result = []\n",
    "        epoch_aux_losses_v = []\n",
    "        epoch_loss_v = 0\n",
    "        iter_c = 0\n",
    "        while not hasFinishEpoch:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_input, gt_aux, gt_final, ptr, hasFinishEpoch = loadInBatch(trainset, ptr, train_batch, shuffle=True)\n",
    "            pred_final, pred_aux = model(batch_input)\n",
    "            \n",
    "            iloss = 0\n",
    "            total_loss = final_loss(pred_final, gt_final)\n",
    "            for i_aux in range(n_aux):\n",
    "                aux_loss = losses[i_aux](pred_aux[i_aux], gt_aux[i_aux])\n",
    "                total_loss += aux_scaler * aux_loss\n",
    "                aux_loss_v = aux_loss.item()\n",
    "                if epoch_aux_losses_v == []:\n",
    "                    epoch_aux_losses_v.append(aux_loss_v)\n",
    "                else:\n",
    "                    epoch_aux_losses_v[iloss] += aux_loss_v\n",
    "                iloss += 1\n",
    "            fine_loss = losses[-1](pred_aux[-1], gt_final)\n",
    "            total_loss += fine_loss\n",
    "            fine_loss_v = fine_loss.item()\n",
    "            if len(epoch_aux_losses_v) <= iloss:\n",
    "                epoch_aux_losses_v.append(fine_loss_v)\n",
    "            else:\n",
    "                epoch_aux_losses_v[iloss] += fine_loss_v\n",
    "            # compute gradients\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iter_c == 0:\n",
    "                epoch_loss_v = total_loss.item()\n",
    "            else:\n",
    "                epoch_loss_v += total_loss.item()\n",
    "            \n",
    "            if epoch_loss_v == 0:\n",
    "                epoch_loss_v = total_loss\n",
    "            \n",
    "            result = computeBatchAccuracy([pred_final],[gt_final])\n",
    "            if epoch_result == []:\n",
    "                epoch_result = result\n",
    "            else:\n",
    "                epoch_result = accumulateList(epoch_result, result)\n",
    "            iter_c += 1\n",
    "        \n",
    "        \n",
    "        #print('Training Loss:', end='')\n",
    "        plot_loss = {}\n",
    "        for iloss in range(n_aux+1):\n",
    "            epoch_aux_losses_v[iloss] /= iter_c\n",
    "            plot_loss[aux_loss_names[iloss]] = epoch_aux_losses_v[iloss]\n",
    "            #print('%s: %f, '%(aux_loss_names[iloss], epoch_aux_losses_v[iloss]), end='')\n",
    "        epoch_loss_v /= iter_c\n",
    "        plot_loss['total loss'] = epoch_loss_v\n",
    "        writer.add_scalars('training loss', \n",
    "                          plot_loss,\n",
    "                          i)\n",
    "        print(plot_loss)\n",
    "        #print('Fine loss: %f'%epoch_loss_v)\n",
    "        \n",
    "        # validation phase\n",
    "        if i % val_at == 0:\n",
    "            print('Validating...')\n",
    "            backbone.eval()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_result, aux_val_result = computeAccuracy(valset, model, val_batch, withAux=True)\n",
    "                for iacc in range(len(aux_val_names)):\n",
    "                    aux_accuracy[aux_val_names[iacc]] = aux_val_result[iacc]\n",
    "                v_result = val_result[0]\n",
    "                print('Validation Accuracy: %f'%v_result)\n",
    "                print(aux_accuracy)\n",
    "                if v_result > best_v_result:\n",
    "                    print('Best model found and saving it.')\n",
    "                    torch.save(model.state_dict(), output_filepath)\n",
    "                    best_v_result = v_result\n",
    "        if i in lr_steps:\n",
    "            olr = lr\n",
    "            lr *= lr_discount\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('learning rate has been discounted from %f to %f'%(olr, lr))\n",
    "        writer.add_scalars('Auxiliary Accuracy', \n",
    "                          aux_accuracy,\n",
    "                          i)\n",
    "        writer.add_scalar('Final Accuracy', \n",
    "                          v_result,\n",
    "                          i)\n",
    "            \n",
    "    print('Model has been trained.')\n",
    "    model = None\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_path, model_fname)\n",
    "    \n",
    "    train_set = loadData(ds_root_path, training_file)\n",
    "    val_set = loadData(ds_root_path, val_file)\n",
    "    print('Training set has been buffered.')\n",
    "    train(train_set, val_set, label_filepath,\n",
    "          output_path = model_path, output_fname = model_fname, \n",
    "          epoch=300, val_at=5, lr_steps=[100, 200],\n",
    "         train_batch=128, val_batch=64, checkpoint=checkpoint_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #final_y, aux_y = nn(x_)\n",
    "    #print('--------Final Output-----------')\n",
    "    #print(final_y)\n",
    "    #print(final_y.argmax())\n",
    "    #print(final_y)\n",
    "    #print('--------Partial Output---------')\n",
    "    #print(aux_y)\n",
    "    #print(aux_y[-1].argmax())\n",
    "    \n",
    "    #nn.eval()\n",
    "    #with torch.no_grad():\n",
    "    #    y = nn(x_)\n",
    "    #    print(y)\n",
    "    \n",
    "    backbone = None\n",
    "    torch.cuda.empty_cache()\n",
    "    writer.close()\n",
    "    print('Done')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_filepath = '/datasets/vision/cifar100_clean/tree.txt'\n",
    "    #label_filepath = '/datasets/dummy/set1/tree.txt'\n",
    "    classTree, n_coarst, coarst_dims = trees.build_itree(label_filepath)\n",
    "    lookup_lv_list = [i+1 for i in range(n_coarst)]\n",
    "    n_fine = len(list(classTree.keys()))\n",
    "    \n",
    "    ds_root_path = '/datasets/vision/cifar100_clean'\n",
    "    training_file = '/datasets/vision/cifar100_clean/train.txt'\n",
    "    val_file = '/datasets/vision/cifar100_clean/val.txt'\n",
    "    test_file = '/datasets/vision/cifar100_clean/val.txt'\n",
    "    \n",
    "    model_path = '/models/cifar100_htcnn_1'\n",
    "    if not os.path.isdir(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    model_fname = 'model.pth'\n",
    "    \n",
    "    backbone = LeNet5(n_classes=n_fine).cuda()\n",
    "    \n",
    "    writer = SummaryWriter(log_dir = '../training', purge_step = 0,\n",
    "                          flush_secs = 5)\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
