{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "Training set has been buffered.\n",
      "Loaded from checkpoint /models/cifar100_htcnn_1/model.pth\n",
      "Validation Accuracy: 0.015924\n",
      "{'Coarst 1 loss': 0.6918770869064819, 'Fine loss': 0.6930978415567247, 'total loss': 2.0781195133238497}\n",
      "Validating...\n",
      "Validation Accuracy: 0.018710\n",
      "{'Level 1 accuracy': 0.07056130573248408, 'Fine accuracy': 0.011345541401273885}\n",
      "Best model found and saving it.\n",
      "{'Coarst 1 loss': 0.6918485879593188, 'Fine loss': 0.6930979802785322, 'total loss': 2.0780911018781345}\n",
      "{'Coarst 1 loss': 0.6918256976415434, 'Fine loss': 0.6930981472020259, 'total loss': 2.078068351501699}\n",
      "{'Coarst 1 loss': 0.6918037370647616, 'Fine loss': 0.6930982985764819, 'total loss': 2.0780464919936628}\n",
      "{'Coarst 1 loss': 0.6917813866949447, 'Fine loss': 0.6930983632116976, 'total loss': 2.078024142233612}\n",
      "{'Coarst 1 loss': 0.6917600182011304, 'Fine loss': 0.6930984560485995, 'total loss': 2.0780028333444425}\n",
      "Validating...\n",
      "Validation Accuracy: 0.020601\n",
      "{'Level 1 accuracy': 0.07593550955414012, 'Fine accuracy': 0.011046974522292993}\n",
      "Best model found and saving it.\n",
      "{'Coarst 1 loss': 0.691738910077478, 'Fine loss': 0.693098556812462, 'total loss': 2.0779817708008124}\n",
      "{'Coarst 1 loss': 0.6917191887145762, 'Fine loss': 0.6930986283075474, 'total loss': 2.0779620777920385}\n",
      "{'Coarst 1 loss': 0.6916987584984821, 'Fine loss': 0.693098682271855, 'total loss': 2.077941652454074}\n",
      "{'Coarst 1 loss': 0.6916789061882916, 'Fine loss': 0.6930987487363693, 'total loss': 2.0779218240772064}\n",
      "{'Coarst 1 loss': 0.6916585002104034, 'Fine loss': 0.6930987969078981, 'total loss': 2.077901409715033}\n",
      "Validating...\n",
      "Validation Accuracy: 0.020701\n",
      "{'Level 1 accuracy': 0.09494426751592357, 'Fine accuracy': 0.007464171974522293}\n",
      "Best model found and saving it.\n",
      "{'Coarst 1 loss': 0.6916377760870073, 'Fine loss': 0.6930988702322821, 'total loss': 2.077880700226025}\n",
      "{'Coarst 1 loss': 0.6916201283864658, 'Fine loss': 0.6930989083426687, 'total loss': 2.0778630511535097}\n",
      "{'Coarst 1 loss': 0.6916032148444134, 'Fine loss': 0.6930989370016796, 'total loss': 2.07784611123907}\n",
      "{'Coarst 1 loss': 0.6915877185514211, 'Fine loss': 0.6930989488921202, 'total loss': 2.077830576835691}\n",
      "{'Coarst 1 loss': 0.6915744463805957, 'Fine loss': 0.6930989423371336, 'total loss': 2.0778172577128693}\n",
      "Validating...\n",
      "Validation Accuracy: 0.018511\n",
      "{'Level 1 accuracy': 0.10310509554140128, 'Fine accuracy': 0.00686703821656051}\n",
      "{'Coarst 1 loss': 0.6915608635338981, 'Fine loss': 0.6930989222148495, 'total loss': 2.0778036227311625}\n",
      "{'Coarst 1 loss': 0.691548574001283, 'Fine loss': 0.6930989095622011, 'total loss': 2.0777912853319016}\n",
      "{'Coarst 1 loss': 0.6915342492215774, 'Fine loss': 0.6930989020925653, 'total loss': 2.077776935094458}\n",
      "{'Coarst 1 loss': 0.6915233688586203, 'Fine loss': 0.6930988874581768, 'total loss': 2.0777659922304665}\n",
      "{'Coarst 1 loss': 0.6915108595052948, 'Fine loss': 0.6930988495002317, 'total loss': 2.077753423729821}\n",
      "Validating...\n",
      "Validation Accuracy: 0.019904\n",
      "{'Level 1 accuracy': 0.10330414012738853, 'Fine accuracy': 0.006568471337579618}\n",
      "{'Coarst 1 loss': 0.6915022593629939, 'Fine loss': 0.6930988482806993, 'total loss': 2.0777447973675742}\n",
      "{'Coarst 1 loss': 0.6914900096176225, 'Fine loss': 0.6930988697749575, 'total loss': 2.0777325355793206}\n",
      "{'Coarst 1 loss': 0.6914814113046203, 'Fine loss': 0.6930988737384377, 'total loss': 2.0777239092170734}\n",
      "{'Coarst 1 loss': 0.6914709277470094, 'Fine loss': 0.6930988248047012, 'total loss': 2.077713356603442}\n",
      "{'Coarst 1 loss': 0.6914610499921052, 'Fine loss': 0.6930987984323136, 'total loss': 2.0777034393661773}\n",
      "Validating...\n",
      "Validation Accuracy: 0.021497\n",
      "{'Level 1 accuracy': 0.10638933121019108, 'Fine accuracy': 0.007265127388535032}\n",
      "Best model found and saving it.\n",
      "{'Coarst 1 loss': 0.6914525926875337, 'Fine loss': 0.6930987871516391, 'total loss': 2.0776949428841283}\n",
      "{'Coarst 1 loss': 0.6914468535681819, 'Fine loss': 0.6930987485839278, 'total loss': 2.0776891360807297}\n",
      "{'Coarst 1 loss': 0.691438341537095, 'Fine loss': 0.6930986682472327, 'total loss': 2.0776805249626373}\n",
      "{'Coarst 1 loss': 0.6914323014981302, 'Fine loss': 0.6930986749546607, 'total loss': 2.077674459618376}\n",
      "{'Coarst 1 loss': 0.6914253526026636, 'Fine loss': 0.6930986679423495, 'total loss': 2.077667499747118}\n",
      "Validating...\n",
      "Validation Accuracy: 0.020502\n",
      "{'Level 1 accuracy': 0.10668789808917198, 'Fine accuracy': 0.008459394904458599}\n",
      "{'Coarst 1 loss': 0.6914111337698329, 'Fine loss': 0.6930987013270483, 'total loss': 2.0776532812191704}\n",
      "{'Coarst 1 loss': 0.6914076936214476, 'Fine loss': 0.6930986286124305, 'total loss': 2.077649747624117}\n",
      "{'Coarst 1 loss': 0.6913957630886751, 'Fine loss': 0.6930986629117786, 'total loss': 2.0776378205975}\n",
      "{'Coarst 1 loss': 0.6913950129238235, 'Fine loss': 0.6930986341003262, 'total loss': 2.077637031560054}\n",
      "{'Coarst 1 loss': 0.6913853610872918, 'Fine loss': 0.6930986118438603, 'total loss': 2.0776273374972134}\n",
      "Validating...\n",
      "Validation Accuracy: 0.019407\n",
      "{'Level 1 accuracy': 0.11086783439490445, 'Fine accuracy': 0.00855891719745223}\n",
      "{'Coarst 1 loss': 0.6913874007551871, 'Fine loss': 0.6930985580319944, 'total loss': 2.0776293070419976}\n",
      "{'Coarst 1 loss': 0.6913761638314523, 'Fine loss': 0.693098568245578, 'total loss': 2.0776180483191213}\n",
      "{'Coarst 1 loss': 0.6913655929248351, 'Fine loss': 0.6930986031546922, 'total loss': 2.0776074792418027}\n",
      "{'Coarst 1 loss': 0.6913730989941551, 'Fine loss': 0.693098483488078, 'total loss': 2.077614862900561}\n",
      "{'Coarst 1 loss': 0.6913607527532846, 'Fine loss': 0.6930984668719494, 'total loss': 2.0776024809883684}\n",
      "Validating...\n",
      "Validation Accuracy: 0.018113\n",
      "{'Level 1 accuracy': 0.1035031847133758, 'Fine accuracy': 0.009852707006369426}\n",
      "{'Coarst 1 loss': 0.6913583271033928, 'Fine loss': 0.6930984497984962, 'total loss': 2.0776000150939082}\n",
      "{'Coarst 1 loss': 0.6913954246684414, 'Fine loss': 0.6930983978159287, 'total loss': 2.0776371309519424}\n",
      "{'Coarst 1 loss': 0.6913558516051154, 'Fine loss': 0.6930984555912749, 'total loss': 2.0775975272478653}\n",
      "{'Coarst 1 loss': 0.6913373262985892, 'Fine loss': 0.6930984642804431, 'total loss': 2.0775789690139654}\n",
      "{'Coarst 1 loss': 0.6913352817525644, 'Fine loss': 0.6930984488838469, 'total loss': 2.077576893369865}\n",
      "Validating...\n",
      "Validation Accuracy: 0.019606\n",
      "{'Level 1 accuracy': 0.10847929936305732, 'Fine accuracy': 0.008160828025477707}\n",
      "{'Coarst 1 loss': 0.6913345527770879, 'Fine loss': 0.6930984562010412, 'total loss': 2.077576176894595}\n",
      "{'Coarst 1 loss': 0.6913299106271066, 'Fine loss': 0.6930984138222911, 'total loss': 2.0775714823047218}\n",
      "{'Coarst 1 loss': 0.6913281941352902, 'Fine loss': 0.6930983860779296, 'total loss': 2.077569721299974}\n",
      "{'Coarst 1 loss': 0.6913198377470227, 'Fine loss': 0.6930983994927857, 'total loss': 2.0775613516492917}\n",
      "{'Coarst 1 loss': 0.6913086715561655, 'Fine loss': 0.6930984138222911, 'total loss': 2.0775501831718115}\n",
      "Validating...\n",
      "Validation Accuracy: 0.021397\n",
      "{'Level 1 accuracy': 0.11544585987261147, 'Fine accuracy': 0.00865843949044586}\n",
      "{'Coarst 1 loss': 0.691300829353235, 'Fine loss': 0.6930983834864234, 'total loss': 2.077542301943845}\n",
      "{'Coarst 1 loss': 0.6913085281086699, 'Fine loss': 0.6930984104685771, 'total loss': 2.077550038047459}\n",
      "{'Coarst 1 loss': 0.6912966230336357, 'Fine loss': 0.6930983999501104, 'total loss': 2.077538095166921}\n",
      "{'Coarst 1 loss': 0.6912833388199282, 'Fine loss': 0.6930984426337434, 'total loss': 2.07752482543516}\n",
      "{'Coarst 1 loss': 0.6912882129859437, 'Fine loss': 0.693098437755614, 'total loss': 2.0775297023451236}\n",
      "Validating...\n",
      "Validation Accuracy: 0.021099\n",
      "{'Level 1 accuracy': 0.11803343949044585, 'Fine accuracy': 0.008459394904458599}\n",
      "{'Coarst 1 loss': 0.6912922639676067, 'Fine loss': 0.6930985036103622, 'total loss': 2.0775338304622095}\n",
      "{'Coarst 1 loss': 0.6912742292179781, 'Fine loss': 0.6930984385178217, 'total loss': 2.077515700893939}\n",
      "{'Coarst 1 loss': 0.6912705962310361, 'Fine loss': 0.6930984066575384, 'total loss': 2.077512022784299}\n",
      "{'Coarst 1 loss': 0.6912596864468606, 'Fine loss': 0.6930984414142111, 'total loss': 2.077501123823473}\n",
      "{'Coarst 1 loss': 0.6912635766026919, 'Fine loss': 0.6930984304384198, 'total loss': 2.077505001936422}\n",
      "Validating...\n",
      "Validation Accuracy: 0.019506\n",
      "{'Level 1 accuracy': 0.11723726114649681, 'Fine accuracy': 0.008160828025477707}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Coarst 1 loss': 0.6912481540914082, 'Fine loss': 0.6930984513229116, 'total loss': 2.0774895730225937}\n",
      "{'Coarst 1 loss': 0.6912468475149111, 'Fine loss': 0.693098458944989, 'total loss': 2.0774882614155255}\n",
      "{'Coarst 1 loss': 0.6912390647641838, 'Fine loss': 0.6930984406520033, 'total loss': 2.077480446042307}\n",
      "{'Coarst 1 loss': 0.6912300938840412, 'Fine loss': 0.6930984286091212, 'total loss': 2.077471453210582}\n",
      "{'Coarst 1 loss': 0.691237036681846, 'Fine loss': 0.6930984119929926, 'total loss': 2.0774783886911923}\n",
      "Validating...\n",
      "Validation Accuracy: 0.021895\n",
      "{'Level 1 accuracy': 0.1252985668789809, 'Fine accuracy': 0.009952229299363057}\n",
      "Best model found and saving it.\n",
      "{'Coarst 1 loss': 0.6912214606619247, 'Fine loss': 0.6930983857730465, 'total loss': 2.077462757944756}\n",
      "{'Coarst 1 loss': 0.6912182101508235, 'Fine loss': 0.6930984205297192, 'total loss': 2.0774595286230295}\n",
      "{'Coarst 1 loss': 0.6912119842856131, 'Fine loss': 0.6930984609267291, 'total loss': 2.0774533309595054}\n",
      "{'Coarst 1 loss': 0.6912015877721255, 'Fine loss': 0.693098375864346, 'total loss': 2.0774428295662335}\n",
      "{'Coarst 1 loss': 0.6911956238014924, 'Fine loss': 0.6930984193101868, 'total loss': 2.0774368977607667}\n",
      "Validating...\n",
      "Validation Accuracy: 0.020800\n",
      "{'Level 1 accuracy': 0.12121815286624203, 'Fine accuracy': 0.009554140127388535}\n",
      "{'Coarst 1 loss': 0.6911900464226218, 'Fine loss': 0.6930983827242156, 'total loss': 2.077431269009095}\n",
      "{'Coarst 1 loss': 0.6911975309976837, 'Fine loss': 0.6930983920231499, 'total loss': 2.0774387825480507}\n",
      "{'Coarst 1 loss': 0.6911829395977127, 'Fine loss': 0.6930983747972552, 'total loss': 2.0774241384033045}\n",
      "{'Coarst 1 loss': 0.6911861710536206, 'Fine loss': 0.6930983586384513, 'total loss': 2.077427356139473}\n",
      "{'Coarst 1 loss': 0.69116947550298, 'Fine loss': 0.6930983226622462, 'total loss': 2.0774105930572278}\n",
      "Validating...\n",
      "Validation Accuracy: 0.023786\n",
      "{'Level 1 accuracy': 0.13126990445859874, 'Fine accuracy': 0.009852707006369426}\n",
      "Best model found and saving it.\n",
      "{'Coarst 1 loss': 0.6911644703896759, 'Fine loss': 0.6930983194609737, 'total loss': 2.0774055667545483}\n",
      "{'Coarst 1 loss': 0.6911595209175364, 'Fine loss': 0.6930983473577768, 'total loss': 2.0774006422828224}\n",
      "{'Coarst 1 loss': 0.6911452747976689, 'Fine loss': 0.6930983340953623, 'total loss': 2.077386355461062}\n",
      "{'Coarst 1 loss': 0.6911405274630202, 'Fine loss': 0.6930982898873137, 'total loss': 2.0773815456253795}\n",
      "{'Coarst 1 loss': 0.6911475995312566, 'Fine loss': 0.6930982668686401, 'total loss': 2.0773886134252524}\n",
      "Validating...\n",
      "Validation Accuracy: 0.022492\n",
      "{'Level 1 accuracy': 0.1282842356687898, 'Fine accuracy': 0.00686703821656051}\n",
      "{'Coarst 1 loss': 0.6911273630683684, 'Fine loss': 0.6930983051314683, 'total loss': 2.0773683539436907}\n",
      "{'Coarst 1 loss': 0.6911259834723704, 'Fine loss': 0.69309811015873, 'total loss': 2.0773667740395}\n",
      "{'Coarst 1 loss': 0.6911172644256631, 'Fine loss': 0.69309812296382, 'total loss': 2.0773580476755984}\n",
      "{'Coarst 1 loss': 0.691110807763951, 'Fine loss': 0.6930981583302588, 'total loss': 2.0773515835442504}\n",
      "{'Coarst 1 loss': 0.6911445182302723, 'Fine loss': 0.6930981908003082, 'total loss': 2.077385425567627}\n",
      "Validating...\n",
      "Validation Accuracy: 0.023288\n",
      "{'Level 1 accuracy': 0.13813694267515925, 'Fine accuracy': 0.010947452229299362}\n",
      "{'Coarst 1 loss': 0.6910891346919262, 'Fine loss': 0.693098069456837, 'total loss': 2.0773297868421317}\n",
      "{'Coarst 1 loss': 0.6910875611902808, 'Fine loss': 0.6930980322610997, 'total loss': 2.077328167303139}\n",
      "{'Coarst 1 loss': 0.6910817162764956, 'Fine loss': 0.6930980394258524, 'total loss': 2.0773223196454063}\n",
      "{'Coarst 1 loss': 0.6910729129296129, 'Fine loss': 0.6930980063460367, 'total loss': 2.077313460352476}\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import cv2\n",
    "import models.layers\n",
    "import addons.trees as trees\n",
    "from models.vision import HTCNN, LeNet5\n",
    "\n",
    "def loadData(data_path, data_file):\n",
    "    output = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for ln in f:\n",
    "            fields = ln.rstrip('\\n').split(',')\n",
    "            output.append([os.path.join(data_path,fields[0]), int(fields[1])])\n",
    "    return output\n",
    "            \n",
    "def loadInBatch(ds, r = 0, batchsize = 16, shuffle=False):\n",
    "    output_data = None\n",
    "    aux_labels = []\n",
    "    fine_labels = None\n",
    "    i = 0\n",
    "    ndata = len(ds)\n",
    "    hasDone = False\n",
    "    while i<batchsize:\n",
    "        data_rec = ds[r][0]\n",
    "        img_data = cv2.imread(data_rec)\n",
    "        base_label = ds[r][1]\n",
    "        data_blob = torch.tensor(img_data).float().permute(2,0,1)\n",
    "        if output_data is None:\n",
    "            output_data = torch.zeros(batchsize, img_data.shape[2], img_data.shape[0], img_data.shape[1], device=device)\n",
    "        output_data[i, ...] = data_blob\n",
    "        if aux_labels == []:\n",
    "            j = 0\n",
    "            for lv in lookup_lv_list:\n",
    "                output_label = torch.zeros(batchsize, coarst_dims[j], device=device)\n",
    "                output_label.require_grad = False\n",
    "                aux_labels.append(output_label)\n",
    "                j += 1\n",
    "        if fine_labels is None:\n",
    "            fine_labels = torch.zeros(batchsize, n_fine, device=device)\n",
    "        j = 0\n",
    "        for lv in lookup_lv_list:\n",
    "            up_cls = lookupParent(classTree, base_label, lv)\n",
    "            aux_labels[j].data[i, up_cls] = 1.0\n",
    "            j += 1\n",
    "        fine_labels.data[i, base_label] = 1.0\n",
    "        r += 1\n",
    "        if r >= ndata:\n",
    "            r = 0\n",
    "            hasDone = True\n",
    "            if shuffle:\n",
    "                random.shuffle(ds)\n",
    "        i += 1\n",
    "        \n",
    "    output_data.require_grad = False\n",
    "    fine_labels.require_grad = False\n",
    "    return output_data, aux_labels, fine_labels, r, hasDone\n",
    "        \n",
    "def lookupParent(tree, fine_node, upper_lv=1):\n",
    "    return tree[fine_node][upper_lv-1]\n",
    "\n",
    "def accumulateList(list1, list2):\n",
    "    output = []\n",
    "    for i in range(len(list1)):\n",
    "        output.append((list1[i] + list2[i]) * 0.5)\n",
    "    return output\n",
    "\n",
    "def computeBatchAccuracy(pred, expected):\n",
    "    output = []\n",
    "    n_output = len(pred)\n",
    "    n_batch = pred[0].shape[0]\n",
    "    for i in range(n_output):\n",
    "        local_result = 0.0\n",
    "        for j in range(n_batch):\n",
    "            cls_pred = pred[i][j].argmax()\n",
    "            cls_exp = expected[i][j,...].argmax()\n",
    "            #print((cls_pred, cls_exp))\n",
    "            if cls_pred == cls_exp:\n",
    "                local_result += 1.0\n",
    "        local_result /= n_batch\n",
    "        output.append(local_result)\n",
    "    return output\n",
    "\n",
    "def computeAccuracy(dataset, model, batchsize = 1, withAux = False):\n",
    "    data_count = len(dataset)\n",
    "    ptr = 0\n",
    "    batch_len = int(np.floor(float(data_count)/batchsize))\n",
    "    batch_elen = int(np.ceil(float(data_count)/batchsize))\n",
    "    output = []\n",
    "    aux_output = []\n",
    "    for i in range(batch_len):\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch(dataset, ptr, batchsize)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        if output == []:\n",
    "            output = batch_result\n",
    "        else:\n",
    "            for j in range(len(output)):\n",
    "                output[j] += batch_result[j]\n",
    "        if withAux:\n",
    "            batch_aux_result = computeBatchAccuracy(pred_aux, expected_aux + [expected_fine])\n",
    "            if aux_output == []:\n",
    "                aux_output = batch_aux_result\n",
    "            else:\n",
    "                for j in range(len(aux_output)):\n",
    "                    aux_output[j] += batch_aux_result[j]\n",
    "    if batchsize!=1 and batch_len != batch_elen:\n",
    "        tmp_batchsize = data_count - ptr\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch(dataset, ptr, tmp_batchsize)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        for j in range(len(output)):\n",
    "            output[j] += batch_result[j]\n",
    "            output[j] /= batch_len + 1\n",
    "        if withAux:\n",
    "            batch_aux_result = computeBatchAccuracy(pred_aux, expected_aux + [expected_fine])\n",
    "            for j in range(len(aux_output)):\n",
    "                aux_output[j] /= batch_len + 1\n",
    "    else:\n",
    "        for j in range(len(output)):\n",
    "            output[j] /= data_count\n",
    "        if withAux:\n",
    "            for j in range(len(aux_output)):\n",
    "                aux_output[j] /= data_count\n",
    "        \n",
    "    return output, aux_output\n",
    "\n",
    "def train(trainset, valset, label_file, output_path, output_fname, \n",
    "          start_lr=0.1, lr_discount=0.1, lr_steps=[], epoch=30,\n",
    "          train_batch = 16, val_batch = 16, val_at = 10,\n",
    "          checkpoint = None, jud_at = -1):\n",
    "    \n",
    "    best_v_result = 0.0\n",
    "    model = HTCNN(label_file, with_aux = True, with_fc = True, backbone=backbone,\n",
    "              isCuda=True).cuda()\n",
    "    \n",
    "    \n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name)\n",
    "    \n",
    "    output_filepath = os.path.join(output_path, output_fname)\n",
    "\n",
    "    if checkpoint is not None and os.path.isfile(checkpoint):\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "        print('Loaded from checkpoint %s'%checkpoint)\n",
    "    \n",
    "    #sample, _, _, _, _ = loadInBatch(trainset, batchsize = 1)\n",
    "    #writer.add_graph(model, sample)\n",
    "    #writer.close()\n",
    "    \n",
    "    v_result = 0\n",
    "    \n",
    "    backbone.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_result, aux_val_result = computeAccuracy(valset, model, val_batch, withAux=True)\n",
    "        v_result = val_result[0]\n",
    "        print('Validation Accuracy: %f'%v_result)\n",
    "        #print(aux_val_result)\n",
    "        best_v_result = v_result\n",
    "    \n",
    "    lr = start_lr\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    \n",
    "    # create losses\n",
    "    losses = []\n",
    "    aux_loss_names = []\n",
    "    aux_val_names = []\n",
    "    final_loss = nn.SoftMarginLoss()\n",
    "    for lv in lookup_lv_list:\n",
    "        losses.append(nn.SoftMarginLoss())\n",
    "        aux_loss_names.append('Coarst %d loss'%lv)\n",
    "        aux_val_names.append('Level %d accuracy'%lv)\n",
    "    losses.append(nn.SoftMarginLoss())\n",
    "    aux_loss_names.append('Fine loss')\n",
    "    aux_val_names.append('Fine accuracy')\n",
    "    n_aux = len(losses) - 1\n",
    "    aux_accuracy = {}\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        # training phase\n",
    "        backbone.train()\n",
    "        model.train()\n",
    "        ptr = 0\n",
    "        hasFinishEpoch = False\n",
    "        epoch_result = []\n",
    "        epoch_aux_losses_v = []\n",
    "        epoch_loss_v = 0\n",
    "        iter_c = 0\n",
    "        while not hasFinishEpoch:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_input, gt_aux, gt_final, ptr, hasFinishEpoch = loadInBatch(trainset, ptr, train_batch, shuffle=True)\n",
    "            pred_final, pred_aux = model(batch_input)\n",
    "            \n",
    "            iloss = 0\n",
    "            total_loss = final_loss(pred_final, gt_final)\n",
    "            for i_aux in range(n_aux):\n",
    "                aux_loss = losses[i_aux](pred_aux[i_aux], gt_aux[i_aux])\n",
    "                total_loss += aux_loss\n",
    "                aux_loss_v = aux_loss.item()\n",
    "                if epoch_aux_losses_v == []:\n",
    "                    epoch_aux_losses_v.append(aux_loss_v)\n",
    "                else:\n",
    "                    epoch_aux_losses_v[iloss] += aux_loss_v\n",
    "                iloss += 1\n",
    "            fine_loss = losses[-1](pred_aux[-1], gt_final)\n",
    "            total_loss += fine_loss\n",
    "            fine_loss_v = fine_loss.item()\n",
    "            if len(epoch_aux_losses_v) <= iloss:\n",
    "                epoch_aux_losses_v.append(fine_loss_v)\n",
    "            else:\n",
    "                epoch_aux_losses_v[iloss] += fine_loss_v\n",
    "            # compute gradients\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iter_c == 0:\n",
    "                epoch_loss_v = total_loss.item()\n",
    "            else:\n",
    "                epoch_loss_v += total_loss.item()\n",
    "            \n",
    "            if epoch_loss_v == 0:\n",
    "                epoch_loss_v = total_loss\n",
    "            \n",
    "            result = computeBatchAccuracy([pred_final],[gt_final])\n",
    "            if epoch_result == []:\n",
    "                epoch_result = result\n",
    "            else:\n",
    "                epoch_result = accumulateList(epoch_result, result)\n",
    "            iter_c += 1\n",
    "        \n",
    "        \n",
    "        #print('Training Loss:', end='')\n",
    "        plot_loss = {}\n",
    "        for iloss in range(n_aux+1):\n",
    "            epoch_aux_losses_v[iloss] /= iter_c\n",
    "            plot_loss[aux_loss_names[iloss]] = epoch_aux_losses_v[iloss]\n",
    "            #print('%s: %f, '%(aux_loss_names[iloss], epoch_aux_losses_v[iloss]), end='')\n",
    "        epoch_loss_v /= iter_c\n",
    "        plot_loss['total loss'] = epoch_loss_v\n",
    "        writer.add_scalars('training loss', \n",
    "                          plot_loss,\n",
    "                          i)\n",
    "        print(plot_loss)\n",
    "        #print('Fine loss: %f'%epoch_loss_v)\n",
    "        \n",
    "        # validation phase\n",
    "        if i % val_at == 0:\n",
    "            print('Validating...')\n",
    "            backbone.eval()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_result, aux_val_result = computeAccuracy(valset, model, val_batch, withAux=True)\n",
    "                for iacc in range(len(aux_val_names)):\n",
    "                    aux_accuracy[aux_val_names[iacc]] = aux_val_result[iacc]\n",
    "                v_result = val_result[0]\n",
    "                print('Validation Accuracy: %f'%v_result)\n",
    "                print(aux_accuracy)\n",
    "                if v_result > best_v_result:\n",
    "                    print('Best model found and saving it.')\n",
    "                    torch.save(model.state_dict(), output_filepath)\n",
    "                    best_v_result = v_result\n",
    "        if i in lr_steps:\n",
    "            olr = lr\n",
    "            lr *= lr_discount\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('learning rate has been discounted from %f to %f'%(olr, lr))\n",
    "        writer.add_scalars('Accuracy', \n",
    "                          aux_accuracy,\n",
    "                          i)\n",
    "            \n",
    "    print('Model has been trained.')\n",
    "    model = None\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_path, model_fname)\n",
    "    \n",
    "    train_set = loadData(ds_root_path, training_file)\n",
    "    val_set = loadData(ds_root_path, val_file)\n",
    "    print('Training set has been buffered.')\n",
    "    train(train_set, val_set, label_filepath,\n",
    "          output_path = model_path, output_fname = model_fname, \n",
    "          epoch=300, val_at=5, lr_steps=[100, 200],\n",
    "         train_batch=128, val_batch=64, checkpoint=checkpoint_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #final_y, aux_y = nn(x_)\n",
    "    #print('--------Final Output-----------')\n",
    "    #print(final_y)\n",
    "    #print(final_y.argmax())\n",
    "    #print(final_y)\n",
    "    #print('--------Partial Output---------')\n",
    "    #print(aux_y)\n",
    "    #print(aux_y[-1].argmax())\n",
    "    \n",
    "    #nn.eval()\n",
    "    #with torch.no_grad():\n",
    "    #    y = nn(x_)\n",
    "    #    print(y)\n",
    "    \n",
    "    backbone = None\n",
    "    torch.cuda.empty_cache()\n",
    "    writer.close()\n",
    "    print('Done')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_filepath = '/datasets/vision/cifar100_clean/tree.txt'\n",
    "    #label_filepath = '/datasets/dummy/set1/tree.txt'\n",
    "    classTree, n_coarst, coarst_dims = trees.build_itree(label_filepath)\n",
    "    lookup_lv_list = [i+1 for i in range(n_coarst)]\n",
    "    n_fine = len(list(classTree.keys()))\n",
    "    \n",
    "    ds_root_path = '/datasets/vision/cifar100_clean'\n",
    "    training_file = '/datasets/vision/cifar100_clean/train.txt'\n",
    "    val_file = '/datasets/vision/cifar100_clean/val.txt'\n",
    "    test_file = '/datasets/vision/cifar100_clean/val.txt'\n",
    "    \n",
    "    model_path = '/models/cifar100_htcnn_1'\n",
    "    if not os.path.isdir(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    model_fname = 'model.pth'\n",
    "    \n",
    "    backbone = LeNet5(n_classes=n_fine).cuda()\n",
    "    \n",
    "    writer = SummaryWriter(log_dir = '../training', purge_step = 0,\n",
    "                          flush_secs = 5)\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
