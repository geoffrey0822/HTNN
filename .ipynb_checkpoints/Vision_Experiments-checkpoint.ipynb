{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "Training set has been buffered.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3b7f0acb2f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-3b7f0acb2f17>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m     train(train_set, val_set, label_filepath,\n\u001b[1;32m    222\u001b[0m           \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m           epoch=30, val_at=5)\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b7f0acb2f17>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainset, valset, label_file, output_path, output_fname, start_lr, lr_discount, lr_steps, epoch, train_batch, val_batch, val_at, checkpoint, jud_at)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mv_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjud_at\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation Accuracy: %f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mv_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mbest_v_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b7f0acb2f17>\u001b[0m in \u001b[0;36mcomputeAccuracy\u001b[0;34m(dataset, model, batchsize)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_fine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadInBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_batchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mpred_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeBatchAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_final\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexpected_fine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b7f0acb2f17>\u001b[0m in \u001b[0;36mcomputeBatchAccuracy\u001b[0;34m(pred, expected)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mcls_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m#print((cls_pred, cls_exp))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcls_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcls_exp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mlocal_result\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mlocal_result\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import cv2\n",
    "import models.layers\n",
    "import addons.trees as trees\n",
    "from models.vision import HTCNN, LeNet5\n",
    "\n",
    "def loadData(data_path, data_file):\n",
    "    output = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for ln in f:\n",
    "            fields = ln.rstrip('\\n').split(',')\n",
    "            output.append([os.path.join(data_path,fields[0]), int(fields[1])])\n",
    "    return output\n",
    "            \n",
    "def loadInBatch(ds, r = 0, batchsize = 16, shuffle=False):\n",
    "    output_data = None\n",
    "    aux_labels = []\n",
    "    fine_labels = None\n",
    "    i = 0\n",
    "    ndata = len(ds)\n",
    "    hasDone = False\n",
    "    while i<batchsize:\n",
    "        data_rec = ds[r][0]\n",
    "        img_data = cv2.imread(data_rec)\n",
    "        base_label = ds[r][1]\n",
    "        data_blob = torch.tensor(img_data).float().permute(2,0,1)\n",
    "        if output_data is None:\n",
    "            output_data = torch.zeros(batchsize, img_data.shape[2], img_data.shape[0], img_data.shape[1], device=device)\n",
    "        output_data[i, ...] = data_blob\n",
    "        if aux_labels == []:\n",
    "            j = 0\n",
    "            for lv in lookup_lv_list:\n",
    "                output_label = torch.zeros(batchsize, coarst_dims[j], device=device)\n",
    "                aux_labels.append(output_label)\n",
    "                j += 1\n",
    "        if fine_labels is None:\n",
    "            fine_labels = torch.zeros(batchsize, n_fine, device=device)\n",
    "        j = 0\n",
    "        for lv in lookup_lv_list:\n",
    "            up_cls = lookupParent(classTree, base_label, lv)\n",
    "            aux_labels[j].data[i, up_cls] = 1.0\n",
    "            j += 1\n",
    "        fine_labels.data[i, base_label] = 1.0\n",
    "        r += 1\n",
    "        if r >= ndata:\n",
    "            r = 0\n",
    "            hasDone = True\n",
    "            if shuffle:\n",
    "                random.shuffle(ds)\n",
    "        i += 1\n",
    "    return output_data, aux_labels, fine_labels, r, hasDone\n",
    "        \n",
    "def lookupParent(tree, fine_node, upper_lv=1):\n",
    "    return tree[fine_node][upper_lv-1]\n",
    "\n",
    "def accumulateList(list1, list2):\n",
    "    output = []\n",
    "    for i in range(len(list1)):\n",
    "        output.append((list1[i] + list2[i]) * 0.5)\n",
    "    return output\n",
    "\n",
    "def computeBatchAccuracy(pred, expected):\n",
    "    output = []\n",
    "    n_output = len(pred)\n",
    "    n_batch = pred[0].shape[0]\n",
    "    for i in range(n_output):\n",
    "        local_result = 0.0\n",
    "        for j in range(n_batch):\n",
    "            cls_pred = pred[i][j].argmax()\n",
    "            cls_exp = expected[i][j,...].argmax()\n",
    "            #print((cls_pred, cls_exp))\n",
    "            if cls_pred == cls_exp:\n",
    "                local_result += 1.0\n",
    "        local_result /= n_batch\n",
    "        output.append(local_result)\n",
    "    return output\n",
    "\n",
    "def computeAccuracy(dataset, model, batchsize = 1):\n",
    "    data_count = len(dataset)\n",
    "    ptr = 0\n",
    "    batch_len = int(np.floor(float(data_count)/batchsize))\n",
    "    output = []\n",
    "    for i in range(batch_len):\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch(dataset, ptr, batchsize)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        if output == []:\n",
    "            output = batch_result\n",
    "        else:\n",
    "            for j in range(len(output)):\n",
    "                output[j] += batch_result[j]\n",
    "    if batchsize!=1:\n",
    "        tmp_batchsize = data_count - ptr\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch(dataset, ptr, tmp_batchsize)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        for j in range(len(output)):\n",
    "            output[j] += batch_result[j]\n",
    "        output[j] /= batch_len + 1\n",
    "    else:\n",
    "        output[j] /= data_count\n",
    "    return output\n",
    "\n",
    "def train(trainset, valset, label_file, output_path, output_fname, \n",
    "          start_lr=0.1, lr_discount=0.1, lr_steps=[], epoch=30,\n",
    "          train_batch = 16, val_batch = 16, val_at = 10,\n",
    "          checkpoint = None, jud_at = -1):\n",
    "    \n",
    "    best_v_result = 0.0\n",
    "    model = HTCNN(label_file, with_aux = True, with_fc = True, backbone=backbone,\n",
    "              isCuda=True).cuda()\n",
    "    \n",
    "    output_filepath = os.path.join(output_path, output_fname)\n",
    "\n",
    "    if checkpoint is not None and os.path.isfile(checkpoint):\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    backbone.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        v_result = computeAccuracy(valset, model, val_batch)[jud_at]\n",
    "        print('Validation Accuracy: %f'%v_result)\n",
    "        best_v_result = v_result\n",
    "    \n",
    "    lr = start_lr\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    # create losses\n",
    "    losses = []\n",
    "    aux_loss_names = []\n",
    "    final_loss = nn.MSELoss()\n",
    "    for lv in lookup_lv_list:\n",
    "        losses.append(nn.MSELoss())\n",
    "        aux_loss_names.append('coarst loss %d'%lv)\n",
    "    n_aux = len(losses)\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        # training phase\n",
    "        backbone.train()\n",
    "        model.train()\n",
    "        ptr = 0\n",
    "        hasFinishEpoch = False\n",
    "        epoch_result = []\n",
    "        epoch_aux_losses_v = []\n",
    "        epoch_loss_v = 0\n",
    "        iter_c = 0\n",
    "        while not hasFinishEpoch:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_input, gt_aux, gt_final, ptr, hasFinishEpoch = loadInBatch(trainset, ptr, train_batch, shuffle=True)\n",
    "            pred_final, pred_aux = model(batch_input)\n",
    "            \n",
    "            iloss = 0\n",
    "            total_loss = final_loss(pred_final, gt_final)\n",
    "            for i_aux in range(n_aux):\n",
    "                aux_loss = losses[i_aux](pred_aux[i_aux], gt_aux[i_aux])\n",
    "                total_loss += aux_loss\n",
    "                aux_loss_v = aux_loss.item()\n",
    "                if epoch_aux_losses_v == []:\n",
    "                    epoch_aux_losses_v.append(aux_loss_v)\n",
    "                else:\n",
    "                    epoch_aux_losses_v[iloss] += aux_loss_v\n",
    "                iloss += 1\n",
    "                 \n",
    "            # compute gradients\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iter_c == 0:\n",
    "                epoch_loss_v = total_loss.item()\n",
    "            else:\n",
    "                epoch_loss_v += total_loss.item()\n",
    "            \n",
    "            if epoch_loss_v == 0:\n",
    "                epoch_loss_v = total_loss\n",
    "            \n",
    "            result = computeBatchAccuracy([pred_final],[gt_final])\n",
    "            if epoch_result == []:\n",
    "                epoch_result = result\n",
    "            else:\n",
    "                epoch_result = accumulateList(epoch_result, result)\n",
    "            iter_c += 1\n",
    "        \n",
    "        \n",
    "        print('Training Loss:', end='')\n",
    "        for iloss in range(n_aux):\n",
    "            epoch_aux_losses_v[iloss] /= iter_c\n",
    "            print('%s: %f, '%(aux_loss_names[iloss], epoch_aux_losses_v[iloss]), end='')\n",
    "        epoch_loss_v /= iter_c\n",
    "        print('Fine loss: %f'%epoch_loss_v)\n",
    "        \n",
    "        # validation phase\n",
    "        if i % val_at == 0:\n",
    "            print('Validating...')\n",
    "            backbone.eval()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                v_result = computeAccuracy(valset, model, val_batch)[jud_at]\n",
    "                print('Validation Accuracy: %f'%v_result)\n",
    "                if v_result > best_v_result:\n",
    "                    print('Best model found and saving it.')\n",
    "                    torch.save(model.state_dict(), output_filepath)\n",
    "                    best_v_result = v_result\n",
    "    print('Model has been trained.')\n",
    "    model = None\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    train_set = loadData(ds_root_path, training_file)\n",
    "    val_set = loadData(ds_root_path, val_file)\n",
    "    print('Training set has been buffered.')\n",
    "    train(train_set, val_set, label_filepath,\n",
    "          output_path = model_path, output_fname = model_fname, \n",
    "          epoch=30, val_at=5,\n",
    "         train_batch=64, val_batch=32)\n",
    "    \n",
    "    \n",
    "    #final_y, aux_y = nn(x_)\n",
    "    #print('--------Final Output-----------')\n",
    "    #print(final_y)\n",
    "    #print(final_y.argmax())\n",
    "    #print(final_y)\n",
    "    #print('--------Partial Output---------')\n",
    "    #print(aux_y)\n",
    "    #print(aux_y[-1].argmax())\n",
    "    \n",
    "    #nn.eval()\n",
    "    #with torch.no_grad():\n",
    "    #    y = nn(x_)\n",
    "    #    print(y)\n",
    "    \n",
    "    backbone = None\n",
    "    torch.cuda.empty_cache()\n",
    "    print('Done')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_filepath = '/datasets/vision/cifar100_clean/tree.txt'\n",
    "    #label_filepath = '/datasets/dummy/set1/tree.txt'\n",
    "    classTree, n_coarst, coarst_dims = trees.build_itree(label_filepath)\n",
    "    lookup_lv_list = [i+1 for i in range(n_coarst)]\n",
    "    n_fine = len(list(classTree.keys()))\n",
    "    \n",
    "    ds_root_path = '/datasets/vision/cifar100_clean'\n",
    "    training_file = '/datasets/vision/cifar100_clean/train.txt'\n",
    "    val_file = '/datasets/vision/cifar100_clean/val.txt'\n",
    "    test_file = '/datasets/vision/cifar100_clean/val.txt'\n",
    "    \n",
    "    model_path = '/models/cifar100_htcnn_1'\n",
    "    if not os.path.isdir(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    model_fname = 'model.pth'\n",
    "    \n",
    "    backbone = LeNet5(n_classes=n_fine).cuda()\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
