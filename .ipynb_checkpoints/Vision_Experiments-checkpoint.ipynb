{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import utils\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import models.layers\n",
    "import addons.trees as trees\n",
    "from models.vision import HTCNN, HTCNN_M, HTCNN_M_IN, LeNet5, AlexNet\n",
    "import argparse\n",
    "\n",
    "def loadData(data_path, data_file):\n",
    "    output = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for ln in f:\n",
    "            fields = ln.rstrip('\\n').split(',')\n",
    "            output.append([os.path.join(data_path,fields[0]), int(fields[1])])\n",
    "    return output\n",
    "            \n",
    "def loadInBatch(ds, r = 0, batchsize = 16, shuffle=False, preprocessor=None):\n",
    "    output_data = None\n",
    "    aux_labels = []\n",
    "    fine_labels = None\n",
    "    i = 0\n",
    "    ndata = len(ds)\n",
    "    hasDone = False\n",
    "    while i<batchsize:\n",
    "        data_rec = ds[r][0]\n",
    "        img_data = None\n",
    "        data_blob = None\n",
    "        im_width = backbone_inshape[2]\n",
    "        im_height = backbone_inshape[1]\n",
    "        im_ch = backbone_inshape[0]\n",
    "        if preprocessor is None:\n",
    "            img_data = cv2.imread(data_rec)\n",
    "            if img_data.shape[0] != backbone_inshape[1] or img_data.shape[1] != backbone_inshape[2]:\n",
    "                img_data = cv2.resize(img_data, (backbone_inshape[1], backbone_inshape[2]))\n",
    "            data_blob = torch.tensor(img_data).float().permute(2,0,1)\n",
    "        else:\n",
    "            img_data = Image.open(data_rec)\n",
    "            data_blob = preprocessor(img_data)\n",
    "        base_label = ds[r][1] \n",
    "        if output_data is None:\n",
    "            output_data = torch.zeros(batchsize, im_ch, im_height, im_width, device=device)\n",
    "        output_data[i, ...] = data_blob\n",
    "        if aux_labels == []:\n",
    "            j = 0\n",
    "            for lv in lookup_lv_list:\n",
    "                output_label = torch.zeros(batchsize, coarst_dims[j]).long().to(device)\n",
    "                output_label.require_grad = False\n",
    "                aux_labels.append(output_label)\n",
    "                j += 1\n",
    "        if fine_labels is None:\n",
    "            fine_labels = torch.zeros(batchsize, n_fine).long().to(device)\n",
    "        j = 0\n",
    "        for lv in lookup_lv_list:\n",
    "            up_cls = lookupParent(classTree, base_label, lv)\n",
    "            aux_labels[j].data[i, up_cls] = 1\n",
    "            j += 1\n",
    "        fine_labels.data[i, base_label] = 1\n",
    "        r += 1\n",
    "        if r >= ndata:\n",
    "            r = 0\n",
    "            hasDone = True\n",
    "            if shuffle:\n",
    "                random.shuffle(ds)\n",
    "        i += 1\n",
    "        \n",
    "    #output_data.require_grad = False\n",
    "    fine_labels.require_grad = False\n",
    "    return output_data, aux_labels, fine_labels, r, hasDone\n",
    "\n",
    "def loadInBatch_mblob(ds, r = 0, batchsize = 16, shuffle=False, preprocessors=None, im_sizes=None,\n",
    "                     general_preprocess = None):\n",
    "    output_data = []\n",
    "    aux_labels = []\n",
    "    fine_labels = None\n",
    "    i = 0\n",
    "    n_output = 0\n",
    "    if preprocessors is not None:\n",
    "        n_output = len(preprocessors)\n",
    "    else:\n",
    "        n_output = len(im_sizes)\n",
    "    ndata = len(ds)\n",
    "    hasDone = False\n",
    "    if preprocessors is None:\n",
    "        raise Exception('Preprocessors cannot be empty.')\n",
    "    while i<batchsize:\n",
    "        data_rec = ds[r][0]\n",
    "        img_data = None\n",
    "        data_blob = None\n",
    "        img_data = Image.open(data_rec)\n",
    "        if general_preprocess is not None:\n",
    "            img_data = general_preprocess(img_data)\n",
    "        for i_output in range(n_output):\n",
    "            im_width = im_sizes[i_output][2]\n",
    "            im_height = im_sizes[i_output][1]\n",
    "            im_ch = im_sizes[i_output][0]\n",
    "            local_output_data = None\n",
    "            data_blob = preprocessors[i_output](img_data).unsqueeze(0)\n",
    "            base_label = ds[r][1] \n",
    "            \n",
    "            if output_data != [] and len(output_data)>i_output:\n",
    "                local_output_data = output_data[i_output]\n",
    "            else:\n",
    "                local_output_data = torch.zeros(batchsize, im_ch, im_height, im_width, device=device)\n",
    "                #local_output_data.require_grad = False\n",
    "                output_data.append(local_output_data)\n",
    "            local_output_data[i, ...] = data_blob\n",
    "        if aux_labels == []:\n",
    "            j = 0\n",
    "            for lv in lookup_lv_list:\n",
    "                output_label = torch.zeros(batchsize, coarst_dims[j]).long().to(device)\n",
    "                output_label.require_grad = False\n",
    "                aux_labels.append(output_label)\n",
    "                j += 1\n",
    "        if fine_labels is None:\n",
    "            fine_labels = torch.zeros(batchsize, n_fine).long().to(device)\n",
    "        j = 0\n",
    "        for lv in lookup_lv_list:\n",
    "            up_cls = lookupParent(classTree, base_label, lv)\n",
    "            aux_labels[j].data[i, up_cls] = 1\n",
    "            j += 1\n",
    "        fine_labels.data[i, base_label] = 1\n",
    "        r += 1\n",
    "        if r >= ndata:\n",
    "            r = 0\n",
    "            hasDone = True\n",
    "            if shuffle:\n",
    "                random.shuffle(ds)\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    fine_labels.require_grad = False\n",
    "    return output_data, aux_labels, fine_labels, r, hasDone\n",
    "\n",
    "def lookupParent(tree, fine_node, upper_lv=1):\n",
    "    return tree[fine_node][upper_lv-1]\n",
    "\n",
    "def accumulateList(list1, list2):\n",
    "    output = []\n",
    "    for i in range(len(list1)):\n",
    "        output.append((list1[i] + list2[i]) * 0.5)\n",
    "    return output\n",
    "\n",
    "def computeBatchAccuracy(pred, expected):\n",
    "    output = []\n",
    "    n_output = len(pred)\n",
    "    n_batch = pred[0].shape[0]\n",
    "    for i in range(n_output):\n",
    "        local_result = 0.0\n",
    "        for j in range(n_batch):\n",
    "            cls_pred = pred[i][j].argmax()\n",
    "            cls_exp = expected[i][j,...].argmax()\n",
    "            #print((cls_pred, cls_exp))\n",
    "            if cls_pred == cls_exp:\n",
    "                local_result += 1.0\n",
    "        local_result /= n_batch\n",
    "        output.append(local_result)\n",
    "    return output\n",
    "\n",
    "def computeAccuracy(dataset, model, batchsize = 1, withAux = False, preprocessor = None):\n",
    "    data_count = len(dataset)\n",
    "    ptr = 0\n",
    "    batch_len = int(np.floor(float(data_count)/batchsize))\n",
    "    batch_elen = int(np.ceil(float(data_count)/batchsize))\n",
    "    output = []\n",
    "    aux_output = []\n",
    "    for i in range(batch_len):\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch(dataset, ptr, batchsize, preprocessor=preprocessor)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        if output == []:\n",
    "            output = batch_result\n",
    "        else:\n",
    "            for j in range(len(output)):\n",
    "                output[j] += batch_result[j]\n",
    "        if withAux:\n",
    "            batch_aux_result = computeBatchAccuracy(pred_aux, expected_aux + [expected_fine])\n",
    "            if aux_output == []:\n",
    "                aux_output = batch_aux_result\n",
    "            else:\n",
    "                for j in range(len(aux_output)):\n",
    "                    aux_output[j] += batch_aux_result[j]\n",
    "    if batchsize!=1 and batch_len != batch_elen:\n",
    "        tmp_batchsize = data_count - ptr\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch(dataset, ptr, tmp_batchsize, preprocessor=preprocessor)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        for j in range(len(output)):\n",
    "            output[j] += batch_result[j]\n",
    "            output[j] /= batch_len + 1\n",
    "        if withAux:\n",
    "            batch_aux_result = computeBatchAccuracy(pred_aux, expected_aux + [expected_fine])\n",
    "            for j in range(len(aux_output)):\n",
    "                aux_output[j] /= batch_len + 1\n",
    "    else:\n",
    "        for j in range(len(output)):\n",
    "            output[j] /= data_count\n",
    "        if withAux:\n",
    "            for j in range(len(aux_output)):\n",
    "                aux_output[j] /= data_count\n",
    "        \n",
    "    return output, aux_output\n",
    "\n",
    "def computeAccuracy_m_in(dataset, model, batchsize = 1, withAux = False, preprocessors = None, im_sizes = None):\n",
    "    data_count = len(dataset)\n",
    "    ptr = 0\n",
    "    batch_len = int(np.floor(float(data_count)/batchsize))\n",
    "    batch_elen = int(np.ceil(float(data_count)/batchsize))\n",
    "    output = []\n",
    "    aux_output = []\n",
    "    for i in range(batch_len):\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch_mblob(dataset, ptr, batchsize, preprocessors=preprocessors, im_sizes=im_sizes)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        if output == []:\n",
    "            output = batch_result\n",
    "        else:\n",
    "            for j in range(len(output)):\n",
    "                output[j] += batch_result[j]\n",
    "        if withAux:\n",
    "            batch_aux_result = computeBatchAccuracy(pred_aux, expected_aux + [expected_fine])\n",
    "            if aux_output == []:\n",
    "                aux_output = batch_aux_result\n",
    "            else:\n",
    "                for j in range(len(aux_output)):\n",
    "                    aux_output[j] += batch_aux_result[j]\n",
    "    if batchsize!=1 and batch_len != batch_elen:\n",
    "        tmp_batchsize = data_count - ptr\n",
    "        batch_data, expected_aux, expected_fine, ptr, _ = loadInBatch_mblob(dataset, ptr, tmp_batchsize, preprocessors=preprocessors, im_sizes=im_sizes)\n",
    "        pred_final, pred_aux = model(batch_data)\n",
    "        batch_result = computeBatchAccuracy([pred_final], [expected_fine])\n",
    "        for j in range(len(output)):\n",
    "            output[j] += batch_result[j]\n",
    "            output[j] /= batch_len + 1\n",
    "        if withAux:\n",
    "            batch_aux_result = computeBatchAccuracy(pred_aux, expected_aux + [expected_fine])\n",
    "            for j in range(len(aux_output)):\n",
    "                aux_output[j] /= batch_len + 1\n",
    "    else:\n",
    "        for j in range(len(output)):\n",
    "            output[j] /= data_count\n",
    "        if withAux:\n",
    "            for j in range(len(aux_output)):\n",
    "                aux_output[j] /= data_count\n",
    "        \n",
    "    return output, aux_output\n",
    "\n",
    "def train(trainset, valset, label_file, output_path, output_fname, \n",
    "          start_lr=0.1, lr_discount=0.1, lr_steps=[], epoch=30,\n",
    "          train_batch = 16, val_batch = 16, val_at = 10,\n",
    "          checkpoint = None, jud_at = -1, aux_scaler = 0.3, final_scaler = 1.0,\n",
    "          preprocessor = None):\n",
    "    global backbone\n",
    "    best_v_result = 0.0\n",
    "    model = HTCNN(label_file, with_aux = True, with_fc = True, backbone=backbone,\n",
    "              isCuda=True, isConditionProb=True).cuda()\n",
    "    \n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name)\n",
    "    \n",
    "    output_filepath = os.path.join(output_path, output_fname)\n",
    "\n",
    "    if checkpoint is not None and os.path.isfile(checkpoint):\n",
    "        \n",
    "        #backbone.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "        model.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "        print('Loaded from checkpoint %s'%checkpoint)\n",
    "    \n",
    "    #sample, _, _, _, _ = loadInBatch(trainset, batchsize = 1)\n",
    "    #writer.add_graph(model, sample)\n",
    "    #writer.close()\n",
    "    \n",
    "    v_result = 0\n",
    "    \n",
    "    backbone.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_result, aux_val_result = computeAccuracy(valset, model, val_batch, withAux=True)\n",
    "        v_result = val_result[0]\n",
    "        print('Validation Accuracy: %f'%v_result)\n",
    "        print(aux_val_result)\n",
    "        best_v_result = v_result\n",
    "    \n",
    "    lr = start_lr\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "    #optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    # create losses\n",
    "    losses = []\n",
    "    aux_loss_names = []\n",
    "    aux_val_names = []\n",
    "    final_loss = nn.MultiLabelSoftMarginLoss()\n",
    "    for lv in lookup_lv_list:\n",
    "        losses.append(nn.MultiLabelSoftMarginLoss())\n",
    "        aux_loss_names.append('Coarst %d loss'%lv)\n",
    "        aux_val_names.append('Level %d accuracy'%lv)\n",
    "    losses.append(nn.MultiLabelSoftMarginLoss())\n",
    "    aux_loss_names.append('Fine loss')\n",
    "    aux_val_names.append('Fine accuracy')\n",
    "    n_aux = len(losses) - 1\n",
    "    aux_accuracy = {}\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        # training phase\n",
    "        backbone.train()\n",
    "        model.train()\n",
    "        ptr = 0\n",
    "        hasFinishEpoch = False\n",
    "        epoch_result = []\n",
    "        epoch_aux_losses_v = []\n",
    "        epoch_loss_v = 0\n",
    "        iter_c = 0\n",
    "        avg_model_fwd_elapsed_time = 0.0\n",
    "        while not hasFinishEpoch:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pp_start_time = time.time()\n",
    "            batch_input, gt_aux, gt_final, ptr, hasFinishEpoch = loadInBatch(trainset, ptr, train_batch, shuffle=True,\n",
    "                                                                            preprocessor=preprocessor)\n",
    "            pp_elapsed_time = time.time() - pp_start_time\n",
    "            \n",
    "            model_start_time = time.time()\n",
    "            pred_final, pred_aux = model(batch_input)\n",
    "            model_fwd_elapsed_time = time.time() - model_start_time\n",
    "            avg_model_fwd_elapsed_time = (avg_model_fwd_elapsed_time + model_fwd_elapsed_time) / 2.0\n",
    "            \n",
    "            iloss = 0\n",
    "            total_loss = final_loss(pred_final, gt_final)\n",
    "            for i_aux in range(n_aux):\n",
    "                aux_loss = losses[i_aux](pred_aux[i_aux], gt_aux[i_aux]) * aux_scaler\n",
    "                total_loss += aux_loss\n",
    "                aux_loss_v = aux_loss.item()\n",
    "                if epoch_aux_losses_v == []:\n",
    "                    epoch_aux_losses_v.append(aux_loss_v)\n",
    "                else:\n",
    "                    epoch_aux_losses_v[iloss] += aux_loss_v\n",
    "                iloss += 1\n",
    "            fine_loss = losses[-1](pred_aux[-1], gt_final) * final_scaler\n",
    "            total_loss += fine_loss\n",
    "            fine_loss_v = fine_loss.item()\n",
    "            if len(epoch_aux_losses_v) <= iloss:\n",
    "                epoch_aux_losses_v.append(fine_loss_v)\n",
    "            else:\n",
    "                epoch_aux_losses_v[iloss] += fine_loss_v\n",
    "            # compute gradients\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iter_c == 0:\n",
    "                epoch_loss_v = total_loss.item()\n",
    "            else:\n",
    "                epoch_loss_v += total_loss.item()\n",
    "            \n",
    "            if epoch_loss_v == 0:\n",
    "                epoch_loss_v = total_loss\n",
    "            \n",
    "            result = computeBatchAccuracy([pred_final],[gt_final])\n",
    "            if epoch_result == []:\n",
    "                epoch_result = result\n",
    "            else:\n",
    "                epoch_result = accumulateList(epoch_result, result)\n",
    "            iter_c += 1\n",
    "            print('[iteration %d]Data Loading Time:%f seconds; Computation Time:%f seconds'%(iter_c,pp_elapsed_time, model_fwd_elapsed_time))\n",
    "        \n",
    "        #print('Training Loss:', end='')\n",
    "        #print('Training Loss:', end='')\n",
    "        plot_loss = {}\n",
    "        for iloss in range(n_aux+1):\n",
    "            epoch_aux_losses_v[iloss] /= iter_c\n",
    "            #plot_loss[aux_loss_names[iloss]] = epoch_aux_losses_v[iloss]\n",
    "            plotter.plot('loss', 'aux %d'%iloss,'Coarst %d Loss'%iloss, i, epoch_aux_losses_v[iloss])\n",
    "            #print('%s: %f, '%(aux_loss_names[iloss], epoch_aux_losses_v[iloss]), end='')\n",
    "        epoch_loss_v /= iter_c\n",
    "        #lot_loss['total loss'] = epoch_loss_v\n",
    "        plotter.plot('loss', 'total','Total Loss', i, epoch_loss_v)\n",
    "        #writer.add_scalars('training loss', \n",
    "        #                  plot_loss,\n",
    "        #                  i)\n",
    "        #print('Fine loss: %f'%epoch_loss_v)\n",
    "        print(plot_loss)\n",
    "        \n",
    "        # validation phase\n",
    "        if i % val_at == 0:\n",
    "            print('Validating...')\n",
    "            backbone.eval()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_result, aux_val_result = computeAccuracy(valset, model, val_batch, withAux=True)\n",
    "                for iacc in range(len(aux_val_names)):\n",
    "                    aux_accuracy[aux_val_names[iacc]] = aux_val_result[iacc]\n",
    "                v_result = val_result[0]\n",
    "                print('Validation Accuracy: %f'%v_result)\n",
    "                print(aux_accuracy)\n",
    "                if v_result > best_v_result:\n",
    "                    print('Best model found and saving it.')\n",
    "                    torch.save(model.state_dict(), output_filepath)\n",
    "                    best_v_result = v_result\n",
    "        if i in lr_steps:\n",
    "            olr = lr\n",
    "            lr *= lr_discount\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('learning rate has been discounted from %f to %f'%(olr, lr))\n",
    "        for i_aux in range(len(aux_accuracy)):\n",
    "            plotter.plot('acc','aux %d'%(i_aux),'Coarst %d'%(i_aux), i, aux_accuracy[aux_val_names[i_aux]])\n",
    "        plotter.plot('acc','final','Final Accuracy', i, v_result)\n",
    "        #writer.add_scalars('Auxiliary Accuracy', \n",
    "        #                  aux_accuracy,\n",
    "        #                  i)\n",
    "        #writer.add_scalar('Final Accuracy', \n",
    "        #                  v_result,\n",
    "        #                  i)\n",
    "            \n",
    "    print('Model has been trained.')\n",
    "    model = None\n",
    "\n",
    "def train_mb(trainset, valset, label_file, output_path, output_fname, \n",
    "          start_lr=0.1, lr_discount=0.1, lr_steps=[], epoch=30,\n",
    "          train_batch = 16, val_batch = 16, val_at = 10,\n",
    "          checkpoint = None, jud_at = -1, aux_scaler = 0.3, final_scaler = 1.0,\n",
    "          preprocessor = None):\n",
    "    \n",
    "    best_v_result = 0.0\n",
    "    backbones = nn.ModuleList([backbone_1, backbone_2])\n",
    "    model = HTCNN_M(label_file, with_aux = True, with_fc = True, backbones=backbones,\n",
    "              isCuda=True, isConditionProb=True).cuda()\n",
    "    \n",
    "    #for name, param in model.named_parameters():\n",
    "    #    if param.requires_grad:\n",
    "    #        print(name)\n",
    "    \n",
    "    output_filepath = os.path.join(output_path, output_fname)\n",
    "\n",
    "    if checkpoint is not None and os.path.isfile(checkpoint):\n",
    "        \n",
    "        #backbone.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "        model.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "        print('Loaded from checkpoint %s'%checkpoint)\n",
    "    \n",
    "    #sample, _, _, _, _ = loadInBatch(trainset, batchsize = 1)\n",
    "    #writer.add_graph(model, sample)\n",
    "    #writer.close()\n",
    "    \n",
    "    v_result = 0\n",
    "    \n",
    "    for backbone in backbones:\n",
    "        backbone.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_result, aux_val_result = computeAccuracy(valset, model, val_batch, withAux=True)\n",
    "        v_result = val_result[0]\n",
    "        print('Validation Accuracy: %f'%v_result)\n",
    "        print(aux_val_result)\n",
    "        best_v_result = v_result\n",
    "    \n",
    "    \n",
    "    lr = start_lr\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "    #optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    # create losses\n",
    "    losses = []\n",
    "    aux_loss_names = []\n",
    "    aux_val_names = []\n",
    "    final_loss = nn.MultiLabelSoftMarginLoss()\n",
    "    for lv in lookup_lv_list:\n",
    "        losses.append(nn.MultiLabelSoftMarginLoss())\n",
    "        aux_loss_names.append('Coarst %d loss'%lv)\n",
    "        aux_val_names.append('Level %d accuracy'%lv)\n",
    "    losses.append(nn.MultiLabelSoftMarginLoss())\n",
    "    aux_loss_names.append('Fine loss')\n",
    "    aux_val_names.append('Fine accuracy')\n",
    "    n_aux = len(losses) - 1\n",
    "    aux_accuracy = {}\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        # training phase\n",
    "        for backbone in backbones:\n",
    "            backbone.train()\n",
    "        model.train()\n",
    "        ptr = 0\n",
    "        hasFinishEpoch = False\n",
    "        epoch_result = []\n",
    "        epoch_aux_losses_v = []\n",
    "        epoch_loss_v = 0\n",
    "        iter_c = 0\n",
    "        avg_model_fwd_elapsed_time = 0.0\n",
    "        while not hasFinishEpoch:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pp_start_time = time.time()\n",
    "            batch_input, gt_aux, gt_final, ptr, hasFinishEpoch = loadInBatch(trainset, ptr, train_batch, shuffle=True,\n",
    "                                                                            preprocessor=preprocessor)\n",
    "            pp_elapsed_time = time.time() - pp_start_time\n",
    "            \n",
    "            model_start_time = time.time()\n",
    "            pred_final, pred_aux = model(batch_input)\n",
    "            model_fwd_elapsed_time = time.time() - model_start_time\n",
    "            avg_model_fwd_elapsed_time = (avg_model_fwd_elapsed_time + model_fwd_elapsed_time) / 2.0\n",
    "            \n",
    "            iloss = 0\n",
    "            total_loss = final_loss(pred_final, gt_final)\n",
    "            for i_aux in range(n_aux):\n",
    "                aux_loss = losses[i_aux](pred_aux[i_aux], gt_aux[i_aux]) * aux_scaler\n",
    "                total_loss += aux_loss\n",
    "                aux_loss_v = aux_loss.item()\n",
    "                if epoch_aux_losses_v == []:\n",
    "                    epoch_aux_losses_v.append(aux_loss_v)\n",
    "                else:\n",
    "                    epoch_aux_losses_v[iloss] += aux_loss_v\n",
    "                iloss += 1\n",
    "            fine_loss = losses[-1](pred_aux[-1], gt_final) * final_scaler\n",
    "            total_loss += fine_loss\n",
    "            fine_loss_v = fine_loss.item()\n",
    "            if len(epoch_aux_losses_v) <= iloss:\n",
    "                epoch_aux_losses_v.append(fine_loss_v)\n",
    "            else:\n",
    "                epoch_aux_losses_v[iloss] += fine_loss_v\n",
    "            # compute gradients\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iter_c == 0:\n",
    "                epoch_loss_v = total_loss.item()\n",
    "            else:\n",
    "                epoch_loss_v += total_loss.item()\n",
    "            \n",
    "            if epoch_loss_v == 0:\n",
    "                epoch_loss_v = total_loss\n",
    "            \n",
    "            result = computeBatchAccuracy([pred_final],[gt_final])\n",
    "            if epoch_result == []:\n",
    "                epoch_result = result\n",
    "            else:\n",
    "                epoch_result = accumulateList(epoch_result, result)\n",
    "            iter_c += 1\n",
    "            print('[iteration %d]Data Loading Time:%f seconds; Computation Time:%f seconds'%(iter_c,pp_elapsed_time, model_fwd_elapsed_time))\n",
    "        \n",
    "        #print('Training Loss:', end='')\n",
    "        plot_loss = {}\n",
    "        for iloss in range(n_aux+1):\n",
    "            epoch_aux_losses_v[iloss] /= iter_c\n",
    "            plot_loss[aux_loss_names[iloss]] = epoch_aux_losses_v[iloss]\n",
    "            plotter.plot('loss', 'aux %d'%iloss,'Coarst %d Loss'%iloss, i, epoch_aux_losses_v[iloss])\n",
    "            #print('%s: %f, '%(aux_loss_names[iloss], epoch_aux_losses_v[iloss]), end='')\n",
    "        epoch_loss_v /= iter_c\n",
    "        #lot_loss['total loss'] = epoch_loss_v\n",
    "        plotter.plot('loss', 'total','Total Loss', i, epoch_loss_v)\n",
    "        #writer.add_scalars('training loss', \n",
    "        #                  plot_loss,\n",
    "        #                  i)\n",
    "        #print('Fine loss: %f'%epoch_loss_v)\n",
    "        print(plot_loss)\n",
    "        \n",
    "        # validation phase\n",
    "        if i % val_at == 0:\n",
    "            print('Validating...')\n",
    "            for backbone in backbones:\n",
    "                backbone.eval()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_result, aux_val_result = computeAccuracy(valset, model, val_batch, withAux=True)\n",
    "                for iacc in range(len(aux_val_names)):\n",
    "                    aux_accuracy[aux_val_names[iacc]] = aux_val_result[iacc]\n",
    "                v_result = val_result[0]\n",
    "                print('Validation Accuracy: %f'%v_result)\n",
    "                print(aux_accuracy)\n",
    "                if v_result > best_v_result:\n",
    "                    print('Best model found and saving it.')\n",
    "                    torch.save(model.state_dict(), output_filepath)\n",
    "                    best_v_result = v_result\n",
    "        if i in lr_steps:\n",
    "            olr = lr\n",
    "            lr *= lr_discount\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('learning rate has been discounted from %f to %f'%(olr, lr))\n",
    "        for i_aux in range(len(aux_accuracy)):\n",
    "            plotter.plot('acc','aux %d'%(i_aux),'Coarst %d'%(i_aux), i, aux_accuracy[aux_val_names[i_aux]])\n",
    "        plotter.plot('acc','final','Final Accuracy', i, v_result)\n",
    "        #writer.add_scalars('Auxiliary Accuracy', \n",
    "        #                  aux_accuracy,\n",
    "        #                  i)\n",
    "        #writer.add_scalar('Final Accuracy', \n",
    "        #                  v_result,\n",
    "        #                  i)\n",
    "            \n",
    "    print('Model has been trained.')\n",
    "    model = None\n",
    "    \n",
    "def train_mb_in(trainset, valset, label_file, output_path, output_fname, \n",
    "          start_lr=0.1, lr_discount=0.1, lr_steps=[], epoch=30,\n",
    "          train_batch = 16, val_batch = 16, val_at = 10,\n",
    "          checkpoint = None, jud_at = -1, aux_scaler = 0.3, final_scaler = 1.0,\n",
    "          preprocessor = None, im_size = None, general_process = None):\n",
    "    \n",
    "    best_v_result = 0.0\n",
    "    backbones = nn.ModuleList([backbone_1, backbone_2])\n",
    "    model = HTCNN_M_IN(label_file, with_aux = True, with_fc = True, backbones=backbones,\n",
    "              isCuda=True, isConditionProb=True).cuda()\n",
    "    \n",
    "    \n",
    "    output_filepath = os.path.join(output_path, output_fname)\n",
    "\n",
    "    if checkpoint is not None and os.path.isfile(checkpoint):\n",
    "        model.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "        print('Loaded from checkpoint %s'%checkpoint)\n",
    "    \n",
    "    \n",
    "    v_result = 0\n",
    "    \n",
    "    for backbone in backbones:\n",
    "        backbone.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_result, aux_val_result = computeAccuracy_m_in(valset, model, val_batch, withAux=True,\n",
    "                                                         im_sizes = im_size,\n",
    "                                                         preprocessors = preprocessor)\n",
    "        v_result = val_result[0]\n",
    "        print('Validation Accuracy: %f'%v_result)\n",
    "        print(aux_val_result)\n",
    "        best_v_result = v_result\n",
    "    \n",
    "    \n",
    "    lr = start_lr\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "    #optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    # create losses\n",
    "    losses = []\n",
    "    aux_loss_names = []\n",
    "    aux_val_names = []\n",
    "    final_loss = nn.MultiLabelSoftMarginLoss()\n",
    "    for lv in lookup_lv_list:\n",
    "        losses.append(nn.MultiLabelSoftMarginLoss())\n",
    "        aux_loss_names.append('Coarst %d loss'%lv)\n",
    "        aux_val_names.append('Level %d accuracy'%lv)\n",
    "    losses.append(nn.MultiLabelSoftMarginLoss())\n",
    "    aux_loss_names.append('Fine loss')\n",
    "    aux_val_names.append('Fine accuracy')\n",
    "    n_aux = len(losses) - 1\n",
    "    aux_accuracy = {}\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        # training phase\n",
    "        for backbone in backbones:\n",
    "            backbone.train()\n",
    "        model.train()\n",
    "        ptr = 0\n",
    "        hasFinishEpoch = False\n",
    "        epoch_result = []\n",
    "        epoch_aux_losses_v = []\n",
    "        epoch_loss_v = 0\n",
    "        iter_c = 0\n",
    "        avg_model_fwd_elapsed_time = 0.0\n",
    "        \n",
    "        blobs = []\n",
    "        \n",
    "        while not hasFinishEpoch:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pp_start_time = time.time()\n",
    "            batch_input, gt_aux, gt_final, ptr, hasFinishEpoch = loadInBatch_mblob(trainset, ptr, train_batch, shuffle=True,\n",
    "                                                                            preprocessors=preprocessor,\n",
    "                                                                                  im_sizes=im_size,\n",
    "                                                                                  general_preprocess=general_process)\n",
    "            pp_elapsed_time = time.time() - pp_start_time\n",
    "            \n",
    "            \n",
    "            model_start_time = time.time()\n",
    "            pred_final, pred_aux = model(batch_input)\n",
    "            model_fwd_elapsed_time = time.time() - model_start_time\n",
    "            avg_model_fwd_elapsed_time = (avg_model_fwd_elapsed_time + model_fwd_elapsed_time) / 2.0\n",
    "            \n",
    "            iloss = 0\n",
    "            total_loss = final_loss(pred_final, gt_final)\n",
    "            for i_aux in range(n_aux):\n",
    "                aux_loss = losses[i_aux](pred_aux[i_aux], gt_aux[i_aux]) * aux_scaler\n",
    "                total_loss += aux_loss\n",
    "                aux_loss_v = aux_loss.item()\n",
    "                if epoch_aux_losses_v == []:\n",
    "                    epoch_aux_losses_v.append(aux_loss_v)\n",
    "                else:\n",
    "                    epoch_aux_losses_v[iloss] += aux_loss_v\n",
    "                iloss += 1\n",
    "            fine_loss = losses[-1](pred_aux[-1], gt_final) * final_scaler\n",
    "            total_loss += fine_loss \n",
    "            fine_loss_v = fine_loss.item()\n",
    "            if len(epoch_aux_losses_v) <= iloss:\n",
    "                epoch_aux_losses_v.append(fine_loss_v)\n",
    "            else:\n",
    "                epoch_aux_losses_v[iloss] += fine_loss_v\n",
    "            # compute gradients\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iter_c == 0:\n",
    "                epoch_loss_v = total_loss.item()\n",
    "            else:\n",
    "                epoch_loss_v += total_loss.item()\n",
    "            \n",
    "            if epoch_loss_v == 0:\n",
    "                epoch_loss_v = total_loss\n",
    "            \n",
    "            result = computeBatchAccuracy([pred_final],[gt_final])\n",
    "            if epoch_result == []:\n",
    "                epoch_result = result\n",
    "            else:\n",
    "                epoch_result = accumulateList(epoch_result, result)\n",
    "            iter_c += 1\n",
    "            #if iter_c == 1:\n",
    "            print('[iteration %d]Data Loading Time:%f seconds; Computation Time:%f seconds'%(iter_c,pp_elapsed_time, model_fwd_elapsed_time))\n",
    "        \n",
    "        plot_loss = {}\n",
    "        for iloss in range(n_aux+1):\n",
    "            epoch_aux_losses_v[iloss] /= iter_c\n",
    "            plot_loss[aux_loss_names[iloss]] = epoch_aux_losses_v[iloss]\n",
    "            plotter.plot('loss', 'aux %d'%iloss,'Coarst %d Loss'%iloss, i, epoch_aux_losses_v[iloss])\n",
    "        epoch_loss_v /= iter_c\n",
    "        plotter.plot('loss', 'total','Total Loss', i, epoch_loss_v)\n",
    "        print(plot_loss)\n",
    "        \n",
    "        # validation phase\n",
    "        if i % val_at == 0:\n",
    "            print('Validating...')\n",
    "            for backbone in backbones:\n",
    "                backbone.eval()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_result, aux_val_result = computeAccuracy_m_in(valset, model, val_batch, withAux=True,\n",
    "                                                                 im_sizes = im_size,\n",
    "                                                         preprocessors = preprocessor)\n",
    "                for iacc in range(len(aux_val_names)):\n",
    "                    aux_accuracy[aux_val_names[iacc]] = aux_val_result[iacc]\n",
    "                v_result = val_result[0]\n",
    "                print('Validation Accuracy: %f'%v_result)\n",
    "                print(aux_accuracy)\n",
    "                if v_result > best_v_result:\n",
    "                    print('Best model found and saving it.')\n",
    "                    torch.save(model.state_dict(), output_filepath)\n",
    "                    best_v_result = v_result\n",
    "        if i in lr_steps:\n",
    "            olr = lr\n",
    "            lr *= lr_discount\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('learning rate has been discounted from %f to %f'%(olr, lr))\n",
    "        for i_aux in range(len(aux_accuracy)):\n",
    "            plotter.plot('acc','aux %d'%(i_aux),'Coarst %d'%(i_aux), i, aux_accuracy[aux_val_names[i_aux]])\n",
    "        plotter.plot('acc','final','Final Accuracy', i, v_result)\n",
    "        \n",
    "    print('Model has been trained.')\n",
    "    model = None\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    is_cond = args.cond == 1\n",
    "    aux_weight = args.aux_weight\n",
    "    final_weight = args.final_weight\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_path, model_fname)\n",
    "    \n",
    "    train_set = loadData(ds_root_path, training_file)\n",
    "    val_set = loadData(ds_root_path, val_file)\n",
    "    print('Training set has been buffered.')\n",
    "    \n",
    "    if backbone is not None:\n",
    "        train(train_set, val_set, label_filepath,\n",
    "              output_path = model_path, output_fname = model_fname, \n",
    "              epoch=300, val_at=5, lr_steps=[100, 200], aux_scaler=aux_weight, final_scaler=final_weight,\n",
    "             train_batch=512, val_batch=1024, checkpoint=checkpoint_path,\n",
    "              start_lr=0.1,\n",
    "             preprocessor=preprocess, isConditionProb = is_cond)\n",
    "    else:\n",
    "        if isinstance(preprocess, list):\n",
    "            train_mb_in(train_set, val_set, label_filepath,\n",
    "                  output_path = model_path, output_fname = model_fname, \n",
    "                  epoch=300, val_at=5, lr_steps=[100, 200], aux_scaler=aux_weight, final_scaler=final_weight,\n",
    "                 train_batch=512, val_batch=1024, checkpoint=checkpoint_path,\n",
    "                     start_lr=0.1,\n",
    "                 preprocessor=preprocess,\n",
    "                       im_size=input_sizes,\n",
    "                       general_process=gp, isConditionProb = is_cond)\n",
    "        else:\n",
    "            train_mb(train_set, val_set, label_filepath,\n",
    "                  output_path = model_path, output_fname = model_fname, \n",
    "                  epoch=300, val_at=5, lr_steps=[100, 200], aux_scaler=aux_weight, final_scaler=final_weight,\n",
    "                 train_batch=512, val_batch=1024, checkpoint=checkpoint_path,\n",
    "                     start_lr=0.1,\n",
    "                 preprocessor=preprocess, isConditionProb = is_cond)\n",
    "    \n",
    "    backbone_1 = None\n",
    "    backbone_2 = None\n",
    "    torch.cuda.empty_cache()\n",
    "    #writer.close()\n",
    "    print('Done')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Training script for HTNN')\n",
    "    parser.add_argument('--cond', dest='cond', type=int, default=1,\n",
    "                       help='identify it is a condition prob or not')\n",
    "    parser.add_argument('--aux-weight', dest='aux_weight', type=float, default=1.0,\n",
    "                       help='loss weight for auxiliry losses')\n",
    "    parser.add_argument('--final-weight', dest='final_weight', type=float, default=1.0,\n",
    "                       help='loss weight for fused loss')\n",
    "    parser.add_argument('--backbone', dest='backbone_net', default='alexnet',\n",
    "                       help='define the backbone network(s)')\n",
    "    parser.add_argument('--multi-input', dest='mult_in', type=int, default=0,\n",
    "                       help='identify it is multiple input for multiple backbone networks')\n",
    "    parser.add_argument('--data-root', dest='data_root', default='/datasets/vision/cifar100_clean',\n",
    "                       help='define the root path for dataset')\n",
    "    parser.add_argument('--train', dest='train_file', default='/datasets/vision/cifar100_clean/train.txt',\n",
    "                       help='define the training filepath')\n",
    "    parser.add_argument('--val', dest='val_file', default='/datasets/vision/cifar100_clean/val.txt',\n",
    "                       help='define the validation filepath')\n",
    "    parser.add_argument('--test', dest='test_file', default='/datasets/vision/cifar100_clean/val.txt',\n",
    "                       help='define the testing filepath')\n",
    "    parser.add_argument('--tree', dest='tree_file', default='/datasets/vision/cifar100_clean/tree.txt',\n",
    "                       help='define the tree filepath')\n",
    "    parser.add_argument('--dst', dest='dst', default='/models/cifar100_htcnn_alexnet_2',\n",
    "                       help='define the output path')\n",
    "    parser.add_argument('--checkpoint', dest='checkpoint', default=None,\n",
    "                       help='define the checkpoint path')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    #label_filepath = '/datasets/vision/cifar100_clean/tree.txt'\n",
    "    #label_filepath = '/datasets/dummy/set1/tree.txt'\n",
    "    label_filepath = args.tree_file\n",
    "    classTree, n_coarst, coarst_dims = trees.build_itree(label_filepath)\n",
    "    lookup_lv_list = [i+1 for i in range(n_coarst)]\n",
    "    n_fine = len(list(classTree.keys()))\n",
    "    print(coarst_dims)\n",
    "    \n",
    "    #ds_root_path = '/datasets/vision/cifar100_clean'\n",
    "    #training_file = '/datasets/vision/cifar100_clean/train.txt'\n",
    "    #val_file = '/datasets/vision/cifar100_clean/val.txt'\n",
    "    #test_file = '/datasets/vision/cifar100_clean/val.txt'\n",
    "    ds_root_path = args.data_root\n",
    "    training_file = args.train_file\n",
    "    val_file = args.val_file\n",
    "    test_file = args.test_file\n",
    "    \n",
    "    #model_path = '/models/cifar100_htcnn_alexnet_2'\n",
    "    model_path = args.dst\n",
    "    if not os.path.isdir(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    model_fname = 'model.pth'\n",
    "    \n",
    "    #backbone_1 = LeNet5(n_classes=coarst_dims[0]).cuda()\n",
    "    #backbone_2 = LeNet5(n_classes=n_fine).cuda()\n",
    "    backbone_1 = AlexNet(n_classes=coarst_dims[0]).cuda()\n",
    "    backbone_2 = AlexNet(n_classes=n_fine).cuda()\n",
    "    #backbone = backbone_2\n",
    "    backbone = None\n",
    "    backbone_inshape = backbone_2.input_dim\n",
    "    \n",
    "    gp = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5)\n",
    "    ])\n",
    "    \n",
    "    preprocess_1 = transforms.Compose([\n",
    "        transforms.Resize(40),\n",
    "        transforms.CenterCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    preprocess_2 = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    preprocess_3 = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    #preprocess = [preprocess_1, preprocess_2]\n",
    "    preprocess = preprocess_3\n",
    "    input_sizes = [(3,32,32), (3,224,224)]\n",
    "    input_sizes = input_sizes[1]\n",
    "    #writer = SummaryWriter(log_dir = '../training', purge_step = 0,\n",
    "    #                      flush_secs = 5)\n",
    "    \n",
    "    global plotter\n",
    "    plotter = utils.VisdomLinePlotter(env_name='Hierarchy Tree Neural Network')\n",
    "    \n",
    "    try:\n",
    "        main()\n",
    "    except:\n",
    "        backbone_1 = None\n",
    "        backbone_2 = None\n",
    "        model = None\n",
    "        torch.cuda.empty_cache()\n",
    "        print('Exception')\n",
    "        exit(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
